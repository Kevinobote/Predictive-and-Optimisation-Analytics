{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "_pv4KkGDCVHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the excel'\n",
        "\n",
        "# Replace with own version of data\n",
        "\n",
        "url = '/content/drive/MyDrive/POA FY25/a_IBM Telco Customers Churn Datasets.xlsx'\n",
        "\n",
        "excel    = pd.ExcelFile(url)\n",
        "\n",
        "# Lets view the excel\n",
        "excel.sheet_names"
      ],
      "metadata": {
        "id": "TMkfmbTVCYAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHsNPCUz0uFD"
      },
      "outputs": [],
      "source": [
        "# Merge the data\n",
        "\n",
        "df_telecho            = pd.read_excel(excel , sheet_name='Telco_Churn')\n",
        "df_status             = pd.read_excel(excel , sheet_name='Status_Analysis')\n",
        "df_customer_info      = pd.read_excel(excel , sheet_name='Customer_Info')\n",
        "df_loc                = pd.read_excel(excel , sheet_name='Location_Data')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmyAl-uN0uFD"
      },
      "outputs": [],
      "source": [
        "df_status.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0ODoTz80uFD"
      },
      "outputs": [],
      "source": [
        "df_customer_info.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "918uB7bR0uFE"
      },
      "outputs": [],
      "source": [
        "df_loc.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObK7LsW30uFE"
      },
      "outputs": [],
      "source": [
        "cust_info =  df_loc.merge(df_customer_info, on='customer_id',how='left').merge(df_status[['customer_id','churn_value']])\n",
        "df_telecho.rename(columns={\"Customer ID\":\"customer_id\"},inplace=True)\n",
        "df = df_telecho.merge(cust_info, on='customer_id',how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sDRfe5B0uFE"
      },
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO2AS9Rp0uFE"
      },
      "outputs": [],
      "source": [
        "#\n",
        "del df['Count']\n",
        "del df['Quarter']\n",
        "del df['customer_id']\n",
        "del df['zip_code']\n",
        "del df['latitude']\n",
        "del df['longitude']\n",
        "del df['city']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Js2VDIew0uFE"
      },
      "source": [
        "#### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Y_jmKpu0uFF"
      },
      "outputs": [],
      "source": [
        "# df.select_dtypes(include=['object','bool','string'])\n",
        "\n",
        "df.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfcQ49SR0uFF"
      },
      "source": [
        "Both Internet Type and Offer are missing at random. Instead of deleting these records—which could potentially introduce bias into our data distribution—we will handle the missing values by imputing them with a constant value. This approach ensures the integrity of our dataset while allowing us to account for these features consistentl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Cdj0Y3H0uFF"
      },
      "outputs": [],
      "source": [
        "df['Internet Type'] = df['Internet Type'].fillna(\"no internet\")\n",
        "df['Offer']         = df['Offer'].fillna(\"no offer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyGyQch90uFF"
      },
      "outputs": [],
      "source": [
        "numerical_cols   = df.select_dtypes(include=['float','int']).columns\n",
        "categorical_cols = df.select_dtypes(include=['object','bool','string']).columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYVc7XEH0uFF"
      },
      "source": [
        "#### Data Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNHHDdnC0uFF"
      },
      "outputs": [],
      "source": [
        "# df.select_dtypes(include=['object','bool','string'])\n",
        "\n",
        "# Check for cardinality\n",
        "\n",
        "for col in categorical_cols:\n",
        "    print(f'{col} has {df[col].nunique()} No of Elements')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNOGDGAu0uFF"
      },
      "source": [
        "**Many columns have managable elements**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjPqYQDT0uFF"
      },
      "source": [
        "**Now we can encoded, We will use simple one -hot encoder from pandas, however always remember to check for ordinal categorical data if you have such use label encoder**\n",
        "**NB. Some ensemble methods are able to handle categorical data an example boosting models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bkt7FOg30uFF"
      },
      "outputs": [],
      "source": [
        "# Now we can encoded, We will use simple one -hot encoder from pandas, however always remember to check for ordinal categorical data if you have such use label encoder\n",
        "\n",
        "\n",
        "encodedf = pd.get_dummies(df[categorical_cols], dtype=float)\n",
        "df = pd.concat( [df[numerical_cols], encodedf], axis=1)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZcvAhS70uFG"
      },
      "source": [
        "**At this point , we may consider doing feature scaling whoever non mixed models ensemble methods are not sensitive to outliers so we will skip it at this point**\n",
        "**We will revist this later**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-_01jQA0uFG"
      },
      "source": [
        "#### Prepare data for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bFErhBv30uFG"
      },
      "outputs": [],
      "source": [
        "# Check for  imbalance\n",
        "\n",
        "df['churn_value'].value_counts(normalize=True)*100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4EAcnIE-0uFG"
      },
      "source": [
        "**Not Bad!! we have 26% of data labelled as Churn, Introducing imbalance techniques may outway the benefits so we will avoid**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3SZAv7n0uFG"
      },
      "outputs": [],
      "source": [
        "# Lets divide our data into X and y\n",
        "\n",
        "\n",
        "X = df.drop(columns=['churn_value'])\n",
        "y= df['churn_value']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "In_zVUmR0uFG"
      },
      "outputs": [],
      "source": [
        "# The next step is to use a hold-out method for split our train , test OR train , validation and test\n",
        "# At this stage we will use the simplest method One Hold method\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "X_train , X_test, y_train, y_test = train_test_split(X,y , test_size=0.2 , random_state=111)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAjFI9Yb0uFG"
      },
      "source": [
        "**Now our data is ready for demonstrating Bias-Trade off Concepts using Decision Tree and Ensemble**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4ek49gq0uFG"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "# Intialize some errors\n",
        "\n",
        "train_erros = []\n",
        "test_errors = []\n",
        "\n",
        "max_depths = range(1,12)\n",
        "\n",
        "for depth in max_depths:\n",
        "    model = DecisionTreeClassifier(max_depth=depth)\n",
        "    model.fit(X_train,y_train)\n",
        "\n",
        "    # compute some errors\n",
        "\n",
        "    train_erros.append(accuracy_score(y_train,model.predict(X_train)))\n",
        "    test_errors.append(accuracy_score(y_test,model.predict(X_test)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyS0Jcsv0uFG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(max_depths,train_erros,label='Training accuracy',marker='o')\n",
        "plt.plot(max_depths,test_errors,label='Test accuracy',marker='*')\n",
        "plt.xlabel('Tree depth')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGG_eSrt0uFG"
      },
      "source": [
        "**One of challenges of the above method is the use of One Hold out cross validation methods**\n",
        "**Rememeber**\n",
        "\n",
        "1. Its does not give us any insights on model stability acccross different splits\n",
        "2. Bias in random Splits\n",
        "3. Unreliable in small dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ1tcMtL0uFG"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "train_accuracies        = []\n",
        "cross_val_accuracies    = []\n",
        "max_depths              = range(1, 12)\n",
        "\n",
        "\n",
        "for depth in max_depths:\n",
        "    model = DecisionTreeClassifier(max_depth=depth, random_state=42)\n",
        "\n",
        "    cv_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')   # note we are now pass the full data since we are apply K-FOLD CV\n",
        "    cross_val_accuracies.append(np.mean(cv_scores))\n",
        "\n",
        "    model.fit(X, y)\n",
        "    train_accuracies.append(np.mean(model.predict(X) == y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwX3RGbU0uFG"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(max_depths, train_accuracies, label=\"Training Accuracy\", marker=\"o\")\n",
        "plt.plot(max_depths, cross_val_accuracies, label=\"Cross-Validation Accuracy\", marker=\"o\")\n",
        "plt.title(\"Bias-Variance Tradeoff in Decision Trees (with Cross-Validation)\")\n",
        "plt.xlabel(\"Tree Depth\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXZbmEES0uFG"
      },
      "source": [
        "Since we have coded a validation curve from scratch , lets use a validation curve in sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6wbvoom0uFG"
      },
      "source": [
        "**Lets now use a validation curve from yellowbrick**\n",
        "\n",
        "Ensure you have install yellowbrick package using =>`pip install yellowbrick`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWx3unpI0uFH"
      },
      "outputs": [],
      "source": [
        "# Investigating the overfitting and underfitting for max-depth params\n",
        "\n",
        "from yellowbrick.model_selection import ValidationCurve\n",
        "\n",
        "\n",
        "\n",
        "viz = ValidationCurve(\n",
        "    DecisionTreeClassifier(),\n",
        "    param_name=\"max_depth\",\n",
        "    param_range=np.arange(1,11),\n",
        "    cv = 10,\n",
        "    scoring= 'accuracy'\n",
        ")\n",
        "\n",
        "viz.fit(X,y)\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-hWMCVl0uFH"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Investigating the overfitting and underfitting for min samples split params\n",
        "\n",
        "viz = ValidationCurve(\n",
        "        DecisionTreeClassifier(),\n",
        "        param_name=\"min_samples_split\",\n",
        "        param_range=np.arange(2,100),\n",
        "        cv = 10,\n",
        "        scoring= 'accuracy',\n",
        "        n_jobs= 10\n",
        "    )\n",
        "\n",
        "viz.fit(X,y)\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sU6DsvzC0uFH"
      },
      "source": [
        "Now lets train our decision tree with max_depth of 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5lZS__G0uFH"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def check_overfitting(model, X_train, X_test, y_train, y_test):\n",
        "\n",
        "\n",
        "    y_pred_train = model.predict(X_train)\n",
        "    y_pred_test  = model.predict(X_test)\n",
        "\n",
        "    train_score = accuracy_score(y_train, y_pred_train)\n",
        "    test_score  = accuracy_score(y_test, y_pred_test)\n",
        "\n",
        "    # Prepare results dictionary\n",
        "    results = {\"Train Score\": train_score, \"Test Score\": test_score}\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T98fkny0uFH"
      },
      "outputs": [],
      "source": [
        "dt_model = DecisionTreeClassifier(\n",
        "        max_depth= 5,\n",
        "        criterion= 'entropy',\n",
        "        min_samples_split= 100\n",
        "    )\n",
        "\n",
        "dt_model.fit(X_train,y_train)\n",
        "dt_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3THulF7r0uFH"
      },
      "outputs": [],
      "source": [
        "check_overfitting(dt_model, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5ANtMAi0uFH"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "\n",
        "\n",
        "fig, axes = plt.subplots(figsize = (4,4), dpi=1200)\n",
        "tree.plot_tree(dt_model,\n",
        "            #    max_depth=1,\n",
        "               feature_names= X.columns,\n",
        "               class_names= [\"alive\",\"dead\"],\n",
        "               filled=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jETbpH_G0uFH"
      },
      "outputs": [],
      "source": [
        "# Lets see the features with most predictive power\n",
        "\n",
        "from yellowbrick.model_selection import FeatureImportances\n",
        "\n",
        "viz = FeatureImportances(dt_model, topn=10)\n",
        "viz.fit(X, y)\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuM7B3V30uFH"
      },
      "source": [
        "### Ensemble Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-TVqf6t0uFI"
      },
      "source": [
        "### 1. Bagging - Bootstrap Aggregating, is an ensemble method that builds multiple versions of a model (usually decision trees) and combines their predictions to reduce variance and improve accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGrRtYpg0uFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import BaggingClassifier\n",
        "\n",
        "bagging_clf = BaggingClassifier(\n",
        "                            estimator=DecisionTreeClassifier(max_depth=5),\n",
        "                            n_estimators= 1000,\n",
        "                            random_state= 11,\n",
        "                            bootstrap=True,  # is True by default\n",
        "                            n_jobs= -1,\n",
        "                            # verbose=True\n",
        "\n",
        "                        )\n",
        "\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "check_overfitting(bagging_clf, X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-m5ZDAy50uFI"
      },
      "source": [
        "Random Forest is an good example of Bagging Method Lets use too"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpCaDj8B0uFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "                    n_estimators= 100,\n",
        "                    max_depth= 5,\n",
        "                    criterion= 'entropy',\n",
        "                    random_state= 112\n",
        "\n",
        "\n",
        "                )\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "check_overfitting(rf_model, X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zU7ZvQre0uFI"
      },
      "outputs": [],
      "source": [
        "viz = FeatureImportances(rf_model,topn=20)\n",
        "viz.fit(X, y)\n",
        "viz.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiVuQf2g0uFI"
      },
      "source": [
        "2. Votting Classifier-  combines predictions from multiple models to improve overall performance. There are two types of voting:\n",
        "\n",
        " -  Hard Voting: Majority voting, where the final prediction is the class most predicted by the base models.\n",
        " -  Soft Voting: Probability averaging, where the final prediction is based on the average predicted probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRwBOAYg0uFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "\n",
        "clf1 = LogisticRegression(random_state=42,penalty='l2')\n",
        "clf2 = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "clf3 = SVC(probability=True, random_state=42)\n",
        "\n",
        "# Create VotingClassifier with soft voting\n",
        "\n",
        "voting_clf = VotingClassifier(\n",
        "                        estimators=[('lr', clf1), ('dt', clf2), ('svc', clf3)],\n",
        "                        voting='soft'\n",
        "                    )\n",
        "\n",
        "voting_clf.fit(X_train,y_train)\n",
        "\n",
        "check_overfitting(voting_clf, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODwSa7FW0uFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "\n",
        "base_models = [\n",
        "    ('dt', DecisionTreeClassifier(random_state=42,max_depth=5)),\n",
        "    ('svc', SVC(probability=True, random_state=42)),\n",
        "    ('rf', RandomForestClassifier(random_state=42,max_depth=2))\n",
        "]\n",
        "\n",
        "# Define meta-model (level-1)\n",
        "meta_model = LogisticRegression(penalty='l2')\n",
        "\n",
        "# Create StackingClassifier\n",
        "stacking_clf = StackingClassifier(\n",
        "                estimators      = base_models,\n",
        "                final_estimator = meta_model,\n",
        "                cv=5\n",
        "            )\n",
        "\n",
        "# Train the StackingClassifier\n",
        "stacking_clf.fit(X, y)\n",
        "\n",
        "check_overfitting(stacking_clf, X_train, X_test, y_train, y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQz1rQtB0uFI"
      },
      "source": [
        "3.Boosting Models\n",
        "\n",
        "models are trained sequentially, and each new model focuses on correcting the errors of the previous ones. This process combines weak learners (usually shallow decision trees) to form a strong predictive model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fNBmT8T50uFI"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "\n",
        "\n",
        "boosting_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100,  # Number of boosting stages\n",
        "    learning_rate=0.1, # Step size shrinkage\n",
        "    max_depth=3,       # Maximum depth of the trees\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "boosting_clf.fit(X_train, y_train)\n",
        "check_overfitting(boosting_clf, X_train, X_test, y_train, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LCvbirY0uFI"
      },
      "source": [
        "*Lets try a neural network to see if any model will match our Ensemble- Boosting**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isFnznlh0uFI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfvFlCX90uFI"
      },
      "outputs": [],
      "source": [
        "# Build the ANN model\n",
        "ann_model = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),  # Input layer\n",
        "    Dropout(0.2),  # Dropout for regularization\n",
        "    Dense(128, activation='relu'),  # Hidden layer\n",
        "    Dense(256, activation='relu'),  # Hidden layer\n",
        "    Dropout(0.2),\n",
        "    Dense(1, activation='sigmoid')  # Output layer for binary classification\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "ann_model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "                            monitor             ='val_loss',\n",
        "                            patience            =5,\n",
        "                            restore_best_weights=True\n",
        "                        )\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpLqjZj30uFI"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "history = ann_model.fit(X_train,\n",
        "                    y_train,\n",
        "                    validation_data=(X_test, y_test),\n",
        "                    epochs=100,\n",
        "                    batch_size=32,\n",
        "                    callbacks = [early_stopping],\n",
        "                    verbose=1\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i8mG70oP0uFJ"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "train_loss, train_acc = ann_model.evaluate(X_train, y_train, verbose=0)\n",
        "test_loss, test_acc = ann_model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Train Accuracy: {train_acc:.2f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7],\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=boosting_clf,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,\n",
        "                           scoring='accuracy',\n",
        "                           n_jobs=-1)\n",
        "\n",
        "# Fit the model to the training data\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Retrieve the best parameters and model accuracy\n",
        "best_params   = grid_search.best_params_\n",
        "best_model    = grid_search.best_estimator_\n",
        "accuracy_best = accuracy_score(y, best_model.predict(X))\n",
        "\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(f\"Best Model Accuracy: {accuracy_best:.4f}\")"
      ],
      "metadata": {
        "id": "V_nOb0kE7SBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators'  : [50, 100, 200],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth'    : [3, 5, 7],\n",
        "}\n",
        "\n",
        "# Initialize RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=boosting_clf,\n",
        "                                   param_distributions=param_dist,\n",
        "                                   n_iter=10,\n",
        "                                   cv=5,\n",
        "                                   scoring='accuracy',\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Retrieve the best parameters and model accuracy\n",
        "best_params_random   = random_search.best_params_\n",
        "best_model_random    = random_search.best_estimator_\n",
        "accuracy_best_random = accuracy_score(y, best_model_random.predict(X))\n",
        "\n",
        "print(\"Best Parameters (Random Search):\", best_params_random)\n",
        "print(f\"Best Model Accuracy (Random Search): {accuracy_best_random:.4f}\")"
      ],
      "metadata": {
        "id": "7iU7jCvO74KE"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}